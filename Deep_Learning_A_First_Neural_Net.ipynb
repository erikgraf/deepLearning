{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A First Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a shallow neural network to classify MNIST digits based on the Keras framework.\n",
    "\n",
    "Keras is an abstraction on top of deeplearning frameworks such as tensorflow and pytorch.\n",
    "It allows us to design and train neural networks using a high level API.\n",
    "\n",
    "The notebook is based on the https://www.deeplearningillustrated.com/ notebook on a first shallow network. \n",
    "\n",
    "You can run the notebook locally or on Google Colab by pressing the button below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/the-deep-learners/deep-learning-illustrated/blob/master/notebooks/shallow_net_in_keras.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "We install tensorflow 1.9.0.\n",
    "This will be the backend for executing the neural network training. \n",
    "It will also automatically install the Keras API.\n",
    "There are major incompatabilities between different tensorflow versions.\n",
    "In order to preserve compatability with the code on this notebook we force installation of version `1.9.0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y tensorflow=1.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies\n",
    "\n",
    "Run the cell below to load the major dependencies we need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "We will load the MNIST data via keras.\n",
    "It is the same format and composition as we have used before. \n",
    "* 60000 training samples\n",
    "* 10000 dedicated test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Samples\n",
    "\n",
    "The cell below can be used to plot some of the sample numbers from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "plt.figure(figsize=(5,5))\n",
    "for k in range(12):\n",
    "    plt.subplot(3, 4, k+1)\n",
    "    plt.imshow(X_train[random.randrange (0,60000,1)], cmap='Greys')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Encoding\n",
    "\n",
    "The cell below shows us the encoding of the first twelve targets.\n",
    "We can see that the targets are encoded as integer values from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data\n",
    "\n",
    "Minor pre-processing is done on the data.\n",
    "We ensure that the values are stored as 'float32' as opposed to 'integer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also normalize the grayscale values by dividing trough 255. \n",
    "Remember that we are dealing with images and that the individual features are pixel-wise gray scale values between 0 and 255. \n",
    "\n",
    "By dividing by 255 we transform the feature value to the scale in the range [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping to Output Layer\n",
    "\n",
    "In order to map the target values to something that is easier to model as an output-layer,\n",
    "we use an encoding that encodes a number between 0 and 9 in form a 10-dimensional vector of 0 and 1 values.\n",
    "\n",
    "This is a typical approach in deep learning to reduce the network to the number of classes `n` at the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number 7 encoded in the 10-dimensional output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design neural network architecture\n",
    "\n",
    "The cell below defines the architecture of our network. \n",
    "\n",
    "`Sequential()` instantiates the model. The term `Sequential` just expresses that we will add a sequence of `layer`s to the model. \n",
    "\n",
    "In the model below there are three layers defined:\n",
    "\n",
    "* Input Layer: This is defined implicitly as an argument of the first layer. \n",
    "* Dense Layer: The first dense layer is defined with 64 neurons \n",
    "* Dense Layer: A second dense layer which is our output layer and has 10 neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Layer\n",
    "\n",
    "A dense layer is a layer where each neuron is fully connected with the input of the preceding layer. \n",
    "\n",
    "In our case in the first dense layer each neuron is connected to all inputs.\n",
    "The second dense layer (which is also our output layer) is fully connected to the 64 neurons of the first dense layer. \n",
    "\n",
    "Below cells show the number of params per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 784 weights per neuron\n",
    "(64 * 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the weights for the input there is also an additional bias weight per neuron.\n",
    "(64 * 784) + 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(10 * 64) + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure model\n",
    "\n",
    "After we have identified the model architecture we have to do some additional configuration. \n",
    "\n",
    "Most of this configuration can be interpreted as hyperparameters. \n",
    "\n",
    "* loss: is a special function to measure at each training step the size of the error the model makes on its prediction. `mean_squared_error` is a standard function to measure this error in a way that is very well suited for using a mathematical optmization function that helps us to efficiently find the best parameter combination. \n",
    "* SGD: is referring to Stochastic Gradient Descent. This is the mathmatical optimization function that we use in order to find the optimal values for the weights. Remember that a large number of double valued parameters means we have a huge parameter space. That is why we need the optimization function. \n",
    "* lr: That is the learning rate the we define. The value that governs how much a single sample is allowed to contribute. \n",
    "* metrics: Just governs how we measure the success at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "In order to run the training we have to supply the following configuration:\n",
    "* epochs: how many times we want to iterate over the dataset\n",
    "* batch_size: how many samples should be used in each stochastic gradient optmization step. \n",
    "* train and validation datasets\n",
    "* verbose: this defines the level of log output we receive during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, epochs=200, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
